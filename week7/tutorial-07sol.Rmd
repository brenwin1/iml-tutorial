---
title: "ETC3250/5250 Tutorial 8 Solution"
subtitle: "Forests and hyperplanes"
author: "Brenwin, tutorial prepared by Professor Di Cook"
date: "Week 9"
output:
  html_document:
    after_body: tutorial-footer.html
    css: tutorial.css
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  echo = FALSE,
  eval = TRUE,
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)

# --- load libraries
library(emo)
library(tidyverse); ggplot2::theme_set(theme_bw())
library(tidymodels)
library(rpart.plot)
library(patchwork)
```



### Group discussion 

Estimating the maximal margin hyperplane corresponds to maximising (believe me, or work your way through [Sidharth's page](https://medium.com/analytics-vidhya/math-behind-support-vector-machines-642421e45b08))

$$\sum_i{\alpha_i} - \frac12 \sum_i\sum_k\alpha_i\alpha_ky_iy_k\mathbf{x}_i^T\mathbf{x}_k$$
where $\mathbf{x}_i, \mathbf{x}_k$ are two $p$-dimensional data vectors, and the coefficients of the separating hyperplane

$$\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p = 0$$

are computed from the support vectors and weights $\alpha$'s from the optimisation as follows:

$$\mathbf{\beta}_j=\sum_{i=1}^s (\alpha_iy_i)\mathbf{x}_{ij}$$

Now, $i, k = 1, ..., n$ but because only some observations are used to compute $\beta$ most are 0, and we can sum only over $1, ..., s$, where $s$ is the number of support vectors. 

With the kernel trick, 

$$\sum_i{\alpha_i} - \frac12\sum_i\sum_k\alpha_i\alpha_ky_iy_kK(\mathbf{x}_i^T\mathbf{x}_k)$$

Try one kernel function transformation to show that $K(\mathbf{x}_i^T\mathbf{x}_k) = \psi(\mathbf{x}_i)^T\psi(\mathbf{x}_k$. You can think of $psi()$ as transformations of the predictors, $\mathbf{x}$.

Fill in the steps to go from the first line to the last. Note that $p=2$. (You can find all the steps in the lecture notes.)

\begin{align*}
\mathcal{K}(\mathbf{x_i}, \mathbf{x_k}) & =  (1 + \langle \mathbf{x_i}, \mathbf{x_k}\rangle) ^2 \\
                                    & = \left(1 + \sum_{j = 1}^2 x_{ij}x_{kj} \right) ^2 \\
                                    & = (1 + x_{i1}x_{k1} + x_{i2}x_{k2})^2 \\
                                    & =  (1, x_{i1}^2, x_{i2}^2, \sqrt2x_{i1}, \sqrt2x_{i2}, \sqrt2x_{i1}x_{i2})^T(1, x_{k1}^2, x_{k2}^2, \sqrt2x_{k1}, \sqrt2x_{k2}, \sqrt2x_{k1}x_{k2}) \\
                                    & = \langle \psi(\mathbf{x_i}), \psi(\mathbf{x_k}) \rangle
\end{align*}


Have a chat about why this algebraic "trick" is neat.

## `r emo::ji("gear")` Exercises 

### 1. Effect of different variables

#### a. 

Fit the tree to olive oils, using a training split of 2/3, using only regions 2, 3,  and  the predictors linoleic and arachidic. Report the balanced accuracy of the test set, and make a plot of the boundary.

      read in data
```{r}
olive <- readr::read_csv("http://www.ggobi.org/book/data/olive.csv") %>%
  dplyr::rename(name = ...1)

notsouth <- olive %>%
  filter(region != 1) %>%
  select(region, linoleic, arachidic) %>%
  mutate(region = factor(region)) # coerce `y` response into factor
```

      split data into 2/3 training & 1/3 test
```{r}
set.seed(2021)

notsouth_split <- rsample::initial_split(notsouth, 
                                         prop = 2/3, # 2/3 training
                                         strata = region) # stratify by `region`; ensure equal proportion of `region`; in training & test set

# extract training & test set
notsouth_tr <- rsample::training(notsouth_split)
notsouth_ts <- rsample::testing(notsouth_split)
```

      always a good idea to plot our data
```{r}
notsouth_tr %>% 
  ggplot() +
  geom_point(aes(x = linoleic,
                 y = arachidic,
                 colour = region)) +
  colorspace::scale_colour_discrete_qualitative()
```

      specify and fit classification tree
•have a feel and anticipate how model will fit data 
-> here; only 2 predictors
```{r}
# --- model specification
tree_spec <- parsnip::decision_tree() %>% # (I) specify model
  set_engine("rpart") %>% # (II) set engine
  set_mode("classification") # (III) set mode

# fit model on training set
notsouth_tree <- tree_spec %>%
  fit(region ~ ., notsouth_tr)
```

      plot classification tree
```{r}
# --- plot the classification tree
notsouth_tree$fit %>%  # extract `rpart` model fit object
  rpart.plot::prp(type = 3, # draw separate split labels; for left & right directions
                  ni = TRUE, # display node indeces 
                  nn = TRUE, # display node numbers
                  extra = 2, # display classification rate at node
                  box.palette = "RdBu") # palette to colour node boxes
```

      create grid of predictor values -> predict using fitted model -> plot boundary
```{r}
# --- create a grid of predictor values
notsouth_p <- expand_grid(linoleic = seq(from = 440, to = 1500, by = 10), 
                          arachidic = seq(from = 0, to = 105, by = 2)) %>% 
  as_tibble()

notsouth_p  <- notsouth_p  %>%
  # predict `region` with fitted classification tree
  mutate(region_tree = predict(notsouth_tree, notsouth_p )$.pred_class)

# --- plot results (classification tree boundary; overlaid with original data pts.)
ggplot() +
  # plot grid of predictor values; colour by predicted class (create boundary)
  geom_point(aes(x = linoleic, 
                 y = arachidic,
                 color = region_tree), 
             alpha = 0.1,
             data = notsouth_p) +
  # plot original data pts.
  geom_point(aes(x = linoleic,
                 y = arachidic,
                 color = region,
                 shape = region),
             data = notsouth) +
  # themes
  scale_color_brewer("", 
                     palette = "Dark2") +
  theme_bw() + 
  theme(aspect.ratio = 1,
        legend.position = "none") +
  ggtitle("tree")
```

      compute balanced accuracy on test set
```{r}
notsouth_ts_pred <- notsouth_ts %>%
  # predict `region` on test set
  mutate(pred_tree = predict(notsouth_tree, notsouth_ts)$.pred_class)

# compute confusion table/matrix
yardstick::conf_mat(data = notsouth_ts_pred,
                    truth = region,
                    estimate = pred_tree)

# compute balanced accuracy
yardstick::bal_accuracy(data = notsouth_ts_pred,
                        truth = region,
                        estimate = pred_tree)
```

**The balanced accuracy is** `r bal_accuracy(notsouth_ts_pred, region, pred_tree)$.estimate`.

#### b. 

Fit a random forest to the full data, using only linoleic and arachidic as predictors, report the  balanced accuracy for the test set, and make a plot of the boundary.

      specify random forest model
```{r}
# --- specify random forest model
rf_spec <- parsnip::rand_forest() %>% # (I) specify model
  # (II) set engine
  parsnip::set_engine("randomForest", 
                      # model arguments
                      importance = TRUE, # assess importance of predictors
                      proximity = TRUE) %>% # return proximity matrix (see similarity between obs.)
  parsnip::set_mode("classification") # (III) set mode

# fit random forest model
notsouth_rf <- rf_spec %>% 
  parsnip::fit(region ~ ., 
               data = notsouth_tr) 
```

      make predictions on grid of predictor values created above + plot
```{r}
notsouth_p  <- notsouth_p %>% # grid of predictor values; created above
  # make predictions using fitted rf model
  mutate(region_rf = predict(notsouth_rf, notsouth_p)$.pred_class)

# --- plot results (random forest boundary; overlaid with original data pts.)
ggplot() +
  # plot grid of predictor values; colour by predicted class(create boundary)
  geom_point(aes(x = linoleic, 
                 y = arachidic,
                 color = region_rf),
             alpha = 0.1,
              data = notsouth_p) +
  # plot original data pts.
  geom_point(aes(x = linoleic,
                 y = arachidic,
                 color = region,
                 shape = region),
             data = notsouth) +
  # add themes
  scale_color_brewer("", 
                     palette = "Dark2") +
  theme_bw() + 
  theme(aspect.ratio = 1,
        legend.position = "none") +
  ggtitle("forest")
```

      make predictions on test set -> generate confusion matrix & balanced accuracy
```{r}
notsouth_ts_pred <- notsouth_ts %>%
  # predict `region` on test set; using rf model
  mutate(pred_rf = predict(notsouth_rf, notsouth_ts)$.pred_class)

# generate confusion matrix/table
yardstick::conf_mat(data = notsouth_ts_pred,
                    truth = region,
                    estimate = pred_rf)

# compute balanced accuracy
yardstick::bal_accuracy(data = notsouth_ts_pred,
                        truth = region,
                        estimate = pred_rf)
```

**The balanced accuracy is** `r bal_accuracy(notsouth_ts_pred, region, pred_rf)$.estimate`.

#### c. 

Explain the difference between the single tree and random forest boundaries.

**The forest model is ever so slightly curved around the two clusters, whereas the tree is a single split. Note though, that with a different seed, the boundary is sometime boxy at the higher values of arachidic. Using a larger number of trees in the forest might stabilise the result.**

#### d. 
Fit the random forest again to the full set of variables, and compute the variable importance. Describe the order of importance of variables.

      read in data
•now with full set of predictors
```{r}
notsouth_all <- olive %>%
  filter(region != 1) %>%
  select(region, palmitic:arachidic) %>%
  mutate(region = factor(region))
```

      split data into 2/3 training & 1/3 test set
```{r}
set.seed(2021)

# split data into 2/3 training 1/3 test
notsouth_all_split <- rsample::initial_split(data = notsouth_all, 
                                             prop = 2/3, # 2/3 training set
                                             strata = region) # stratify by `region`; ensure equal proportion of `region` in training & test set

# extract training & test set
notsouth_all_tr <- training(notsouth_all_split)
notsouth_all_ts <- testing(notsouth_all_split)
```

      specify & fit the random forest model
```{r}
# --- specify random forest model
rf_spec <- parsnip::rand_forest() %>% # (I) specify model
  # (II) set engine
  parsnip::set_engine("randomForest", 
                      importance = TRUE, # assess importance of predictors
                      proximity = TRUE) %>% # return proximity matrix
  parsnip::set_mode("classification") # (III) set mode

# fit random forest model
notsouth_all_rf <- rf_spec %>% 
  parsnip::fit(region ~ .,
               data = notsouth_all_tr) 
```

      extract variable importance
```{r}
options(digits = 2) # output numbers in 2 significant values

notsouth_all_rf$fit$importance

# --- helper function; plot variable importance scores for predictors in a model
vip::vip(notsouth_all_rf)
```

**The most important variables by far are linoleic and oleic, with arachidic much less important.**

#### e. 

Create a new variable called `linoarch` that is $0.377 \times linoleic + 0.926\times arachidic$. Make a plot of this variable against arachidic. Fit the tree model to the same training data using this variable in addition to linoleic and arachidic. Why doesn't the tree use this new variable? It has a bigger difference between the two groups than linoleic? Change the order of the variables, so that linoarch is before linoleic and re-fit the tree. Does it use this variable now? Why do you think this is?

      create new `linoarch` variable
```{r}
notsouth_tr <- notsouth_tr %>%
  # create new `linoarch` variable
  mutate(linoarch = 0.377*linoleic + 0.926*arachidic)
```

```{r}
# --- `arachidic` vs. `linoleic`
p1 <- notsouth_tr %>% 
  ggplot() +
  geom_point(aes(x = linoleic, 
                 y = arachidic,
                 color = region,
                 shape = region)) +
  # add themes
  scale_color_brewer("", 
                     palette = "Dark2") +
  theme_bw() + 
  theme(aspect.ratio = 1,
        legend.position = "none") +
  ggtitle("tree")

# --- `arachidic` vs. `linoarch`
p2 <- notsouth_tr %>% 
  ggplot() +
  geom_point(aes(x = linoarch, 
                 y = arachidic,
                 color = region,
                 shape = region)) +
  # add themes
  scale_color_brewer("", 
                     palette = "Dark2") +
  theme_bw() + 
  theme(aspect.ratio = 1,
        legend.position = "none") +
  ggtitle("tree")

require(patchwork)
p1 + p2
```

      fit classification tree; with order of variables: `linoleic`, `arachidic`, `linoarch`
```{r}
# --- classification tree model specification
tree_spec <- parsnip::decision_tree() %>% # (I) specify model
  set_engine("rpart") %>% # (II) set engine
  set_mode("classification") # (III) set mode

# fit classification tree
notsouth_tree <- tree_spec %>%
  parsnip::fit(region ~ .,
               data = notsouth_tr)
```

```{r}
# --- plot the classification tree
notsouth_tree$fit %>%  # extract `rpart` model fit object
  rpart.plot::prp(type = 3, # draw separate split labels; for left & right directions
                  ni = TRUE, # display node indeces 
                  nn = TRUE, # display node numbers
                  extra = 2, # display classification rate at node
                  box.palette = "RdBu") # palette to colour node boxes
```

      refit classification tree; changing order of predictors to `linoarch`, `linoleic`, `arachidic`
```{r}
# --- fit classification tree; changing order of predictors
notsouth_tree <- tree_spec %>% 
  parsnip::fit(region ~ linoarch + linoleic + arachidic, 
               data = notsouth_tr)

# --- plot the classification tree
notsouth_tree$fit %>%  # extract `rpart` model fit object
  rpart.plot::prp(type = 3, # draw separate split labels; for left & right directions
                  ni = TRUE, # display node indeces 
                  nn = TRUE, # display node numbers
                  extra = 2, # display classification rate at node
                  box.palette = "RdBu") # palette to colour node boxes

```

**Yes, it sees the new variable when order is changed. The initial fit doesn't see the better variable because both variables have a split with the same impurity value, and thus the first variable entered into the model is the one that is selected. We can see that the new variable is "better" than the first because there is a bigger gap between the two groups, but this isn't a factor considered by a tree model.**

#### f. 

Fit the random forest again to the full set of variables, including linoarch and  compute the variable importance. Describe the order of importance of variables. Does the forest see the new variable?

```{r}
# --- specify random forest model
rf_spec <- parsnip::rand_forest() %>% # (I) specify model
  # (II) set engine
  parsnip::set_engine("randomForest", 
                      importance = TRUE, # assess importance of predictors
                      proximity = TRUE) %>% # return proximity matrix
  parsnip::set_mode("classification") # (III) set mode

# fit the random forest model
notsouth_rf <- rf_spec %>% 
  fit(region ~ .,
      data = notsouth_tr) 
```

      assess variable importance
```{r}
notsouth_rf$fit$importance
```

**Yes, the forest sees the new variable. However, it considers it to be equally as important as linoleic. The forest also doesn't recognise that the new variable is better because it also is not considering the magnitude of the gap between groups.**

### 2. Support vector machine model fitting

#### a. 

Fit the linear SVM to olive oils, using a training split of 2/3, using only regions 2, 3,  and  the predictors linoleic and arachidic. It can be helpful to standardise the variables before fitting svm, and then set `scaled = FALSE` as the argument to the fitting function.

Report the balanced accuracy of the test set, list the support vectors, the coefficients for the support vectors and the equation for the separating hyperplane, and
$$???\times\text{linoleic}+???\times\text{arachidic}+??? > 0$$ and make a plot of the boundary, overlaid by the data with support vectors marked.

      read in data + standardise variables
```{r}
# create function to standardise variables
std <- function(x) {(x - mean(x)) / sd(x)}

notsouth <- olive %>%
  filter(region != 1) %>%
  select(region, linoleic, arachidic) %>%
  mutate(region = factor(region)) %>% # coerce response to factor variable
  # standardise variables
  mutate(across(.cols = where(is.numeric),
                .fns = std))
```

      split data into training & test set
```{r}
set.seed(2021)

notsouth_split <- rsample::initial_split(data = notsouth,
                                         prop = 2/3, # 2/3 training
                                         strata = region) # stratify by `region`; ensure equal proportions of `region` in training & test set

# extract training & test sets
notsouth_tr <- rsample::training(notsouth_split)
notsouth_ts <- rsample::testing(notsouth_split)
```

      specify + fit svm model (using linear boundary) *i.e. support vector classifier
```{r}
# --- SVM model specification
svm_mod <- parsnip::svm_rbf(cost = 10) %>% # (I) specify SVM model// c: costant of regularisation term in Lagrange formulation (i.e. how tolerant we are to misclassification)
  parsnip::set_engine("kernlab", # (II) set engine 
                      # model arguments
                      kernel = "vanilladot", # use linear kernel
                      scaled = FALSE) %>% # FALSE; since we already scaled our predictors
  parsnip::set_mode("classification") # (III) set mode
  
# fit SVM model
notsouth_svm <- svm_mod %>%
  parsnip::fit(region ~ ., 
               data = notsouth_tr)
```
      
      create grid of predictor values + predict using fitted model + plot boundary
```{r}
# --- create grid of predictor values *using min & max values as guide
notsouth_p <- expand_grid(linoleic = seq(-2.2, 2.2, 0.1),
                          arachidic = seq(-2, 2, 0.1)) %>% 
  as_tibble()

# predict using SVM model on grid of predictor values
notsouth_p  <- notsouth_p  %>% # grid of predictor values; created above
  mutate(region_svm = predict(notsouth_svm, notsouth_p )$.pred_class)

# --- plot results
ggplot() +
  # plot grid of predictor values; colour by predicted class (create boundary)
  geom_point(aes(x = linoleic, 
                 y = arachidic,
                 color = region_svm),
             alpha = 0.1,
             data = notsouth_p) +
  # overlay original data pts.
  geom_point(aes(x = linoleic,
                 y = arachidic,
                 color = region,
                 shape = region),
             data = notsouth) +
  # circle support vectors
  geom_point(aes(x = linoleic,
                 y = arachidic),
            shape = 1,
            size = 3,
            colour = "black",
            data = notsouth_tr[notsouth_svm$fit@SVindex,]) +
  # add fitted SVM linear boundary line
  geom_abline(intercept = 1.4, 
              slope = -3.4) +
  # themes
  scale_color_brewer("", palette="Dark2") +
  theme_bw() + 
  theme(aspect.ratio = 1,
        legend.position = "none") +
  ggtitle("SVM") 
```

**The** $\alpha$**'s, indexes of support vectors and** $\beta_0$, and the observations that are the support vectors are:

      extract support vector index
```{r}
notsouth_svm$fit@SVindex # index of support vectors

notsouth_tr[notsouth_svm$fit@SVindex,] # extract support vectors
```
      

```{r}
notsouth_svm$fit@coef # \alpha_i*y_i (weights)

notsouth_svm$fit@b # \beta_0

notsouth_tr[notsouth_svm$fit@SVindex,-1] # extract support vectors (x_{ij}); using index

```

**You need to use the formula**

$$\mathbf{\beta}_j=\sum_{i=1}^s (\alpha_iy_i)\mathbf{x}_{ij}$$

**to compute the remaining coefficients.**

$\beta_1 = -5.2*0.614+2.7*(-0.348)+2.4*0.410$
```{r}
# \beta_1
((notsouth_tr[notsouth_svm$fit@SVindex,-1] %>% pull(linoleic)) * notsouth_svm$fit@coef[[1]]) %>% sum
```


$\beta_2 = -5.2*0.351+2.7*1.63+2.4*(-1.40)$
```{r}
# \beta_2
((notsouth_tr[notsouth_svm$fit@SVindex,-1] %>% pull(arachidic)) * notsouth_svm$fit@coef[[1]]) %>% sum
```


which would give the equation of the separating hyperplane to be

$$1.2 + -3.1484\mbox{linoleic}  -0.7842\mbox{arachidic} = 0$$

and rearranging for arachidic as the focus gives

$$\mbox{arachidic} = 1.2/0.7842 + -3.1484/0.7842\mbox{x linoleic}$$

slope of the line is -4.014792, and intercept is 1.530222, gives the line drawn on the plot above.

####  b. 

Fit a radial kernel SVM, with a variety of cost values, to examine the effect on the boundary.

      specify and fit SVM model (using radial kernel)
```{r}
# --- model specification
svm_mod2 <- parsnip::svm_rbf(cost = 1) %>% # (I) specify SVM model//set C = 1 (very flexible)
  # (II) set engine
  parsnip::set_engine("kernlab", 
                      # model arguments
                      kernel = "rbfdot", # radial basis kernel ("Gaussian")
                      scaled = FALSE) %>% # already standardised variables
  parsnip::set_mode("classification") # (III) set mode

# fit the SVM model; on training set
notsouth_svm2 <- svm_mod2 %>%
  parsnip::fit(region ~ ., 
               data = notsouth_tr)
```

      using SVM model; make predictions on grid of predictor values created above + plot boundary
```{r}
notsouth_p  <- notsouth_p  %>%
  mutate(region_svm2 = predict(notsouth_svm2, notsouth_p)$.pred_class)

ggplot() +
  # plot grid of predictor values; colour by predicted class (create boundary)
  geom_point(aes(x = linoleic,
                 y = arachidic,
                 color = region_svm2), 
             alpha = 0.1,
             data = notsouth_p) +
  # overlay original data pts.
  geom_point(aes(x = linoleic,
                 y = arachidic,
                 color = region,
                 shape = region),
             data = notsouth) +
  # circle support vectors
  geom_point(aes(x = linoleic,
                 y = arachidic),
             shape = 1,
             size = 3,
             colour = "black",
             data = notsouth_tr[notsouth_svm2$fit@SVindex,]) +
  # themes
  scale_color_brewer("", 
                     palette = "Dark2") +
  theme_bw() + 
  theme(aspect.ratio = 1,
        legend.position = "none") +
  ggtitle("SVM")
```

**The boundary is always a small circle wrapping the smaller group. Very low values of cost break the fit, though, and give poor prediction of the smaller group.**

##### © Copyright 2021 Monash University
