---
title: "ETC3250/5250 Tutorial 1 Solution"
subtitle: "Introduction to tidymodels"
author: "prepared by Professor Di Cook"
date: "Week 1"
output:
  html_document:
    after_body: tutorial-footer.html
    css: tutorial.css
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "#",
  echo=TRUE,
  fig.height = 4,
  fig.width = 8,
  out.width = "100%",
  fig.align = "center",
  cache = FALSE
)

library(emo)
```


## Readings

    case for tidymodels
- subset of package of the renowned `tidyverse` package
  - adheres to `tidyverse` syntax & design principles (which many know promote consistency & well-designed human interfaces)
  - works intuitively with `tidyverse` interface; e.g. using the `%>%` operator
=> organised way to fit models

- like `tidyverse`; designed as set of modular R packages; integrated *modular package philosophy
  - `parsnip`: provide fluent & standardised interface; fit variety of models
  - `rsample`: focus on data splitting & resampling
  - `yardstick`: obtain performance metrics; for evaluating models
  - `dials`/`tune`: creating & managing tuning params.
  - `recipe`: carry out preprocessing steps 
  - `workflow`: encapsulate major pieces of modelling process into an object

therefore; less cramped/boated model development// smoother package maintenance

- don't just step through modelling steps, but implement coherent conceptual structures
  - encourage good methodology & statistical practice
    - make complex iterative workflow possible & reproducible
      - separate broad moodelling processes (pre-processing, fitting, post-processing)
    - time saving framework to explore multiple moodels
    - similar syntax across various models
      - don't need; worry; different models; having different syntax 
    - interfaces to common tasks are standardised
  
  

## `r emo::ji("gear")` Exercise 1

The `nrc` data contains information collected on Statistics graduate programs in the USA. There are several ranking variables, and indicators of the departments' describing research, student and diversity, summarising many individual variables such as number of publications, student entrance scores and demographics. You can learn more about this data [here](https://en.wikipedia.org/wiki/United_States_National_Research_Council_rankings).

The goal here is to follow the tidy models approach to fit a model for rank against indicators of research. 

### 1 

Load the libraries to complete the exercises.

```{r}
# --- load libraries
library(tidyverse)
library(tidymodels)
library(broom)
library(dotwhisker)
library(patchwork)
```




### 2

Read the data, simplify the names and select the relevant variables. You will want `rank = R.Rankings.5th.Percentile`, `research = Research.Activity.5th.Percentile`, `student = Student.Support.Outcomes.5th.Percentile` and `diversity = Diversity.5th.Percentile`. 

```{r}
# --- read in data; from website
nrc <- readr::read_csv("https://iml.numbat.space/data/nrc.csv")

# OR
nrc <- readr::read_csv(here::here("data/nrc.csv"))
```

```{r}
# --- simplify variable names & select relevant variables to use
nrc <- nrc %>%
  # create variables
  mutate(rank = R.Rankings.5th.Percentile, # rank (response)
         research = Research.Activity.5th.Percentile, # amt. of Research activity
         student = Student.Support.Outcomes.5th.Percentile, # level of student support outcomes
         diversity = Diversity.5th.Percentile) %>% # level of diversity
  # select relevant ones
  select(rank, research, student, diversity)

# OR use select
# -> LHS: variable name you want// RHS: the variable

nrc <- nrc %>%
  # select relevant variables & rename them
  dplyr::select(rank = R.Rankings.5th.Percentile, # LHS: name// RHS: variable selected
                research = Research.Activity.5th.Percentile,
                student = Student.Support.Outcomes.5th.Percentile,
                diversity = Diversity.5th.Percentile)
```

### 3

Make a plot of the observed response against predictors. What do you learn about the relationship between these variables?

```{r}
# --- plot response against each predictor
a1 <- nrc %>% 
  ggplot(aes(x = research,
             y = rank)) + 
  geom_point() +
  # include smooth line
  geom_smooth(se = FALSE)

a2 <- nrc %>% 
  ggplot(aes(x = student,
             y = rank)) + 
  geom_point() +
  # include smooth line
  geom_smooth(se = FALSE)

a3 <- nrc %>% 
  ggplot(aes(x = diversity,
             y = rank)) + geom_point() +
  # include smooth line
  geom_smooth(se = FALSE) # no standard error band// default: `span = 0.75`// `stat = "smooth"`
  

# patchwork (place plots side by side)
require(patchwork)
a1 + a2 + a3
```

*You can see that the relationship between rank and research is very strong. There is very little relationship with either of the other predictors.*

### 4

Set up the model. While it is unnecessary to set the mode for a linear regression since it can only be regression, we continue to do it in these labs to be explicit. The specification doesn't perform any calculations by itself. It is just a specification of what we want to do.

    3 general steps to set up a model

(I) specify the model *usually based on its mathematical structure
-> e.g. linear regression// decision trees

(II) specify engine for fitting model
-> most often reflects software package that should be used

(III) declare mode of the model (when required)
-> `mode = "regression"`// `mode = "classification"`


```{r}
# --- set up 
lm_mod <- parsnip::linear_reg() %>% # (I) model specification 
  parsnip::set_engine("lm") %>%  # (II) set engine 
  set_mode("regression") # (III) be explicit about mode (either "classification" or "regression")

lm_mod

# -> provide details on  how `parsnip` converts code to package's syntax
lm_mod %>% 
  parsnip::translate()
# *note: `missing_arg()`: placeholder for data; yet to be provided
```

```{r}
# === e.g. similar syntax

# --- linear regression

parsnip::linear_reg() %>% # model specification
  parsnip::set_engine(engine = "lm") %>% # set engine
  set_mode("regression") %>% 
  parsnip::translate() # provide details on how `parsnip` converts code to package's syntax

# --- regression tree (in week 7)
parsnip::rand_forest() %>% # model specification
  parsnip::set_engine(engine = "ranger") %>% # set engine
  parsnip::set_mode("regression") %>% 
  parsnip::translate() # provide details on how `parsnip` converts code to package's syntax
```



### 5

Fit the model. Once we have the specification we can fit it by supplying a formula expression and the data we want to fit the model on. The formula is written on the form `y ~ x` where `y` is the name of the response and `x` is the name of the predictors. The names used in the formula should match the names of the variables in the data set passed to data.

```{r}
lm_fit <- lm_mod %>% 
  parsnip::fit(rank ~ research + student + diversity, # LHS (y) ~ RHS (x)
               data = nrc)

lm_fit
```

The result of this fit is a [parsnip](https://parsnip.tidymodels.org) model object. This object contains the underlying fit as well as some parsnip-specific information. If we want to look at the underlying fit object we can access it and summarise it with 

```{r}
lm_fit %>%
  purrr::pluck("fit") %>% # pluck single element; from `list` = `lm_fit$fit`
  summary()

lm_fit$fit %>% # `lm` object
  summary()
```

### 6

Report the coefficients of the model fit. We can use packages from the broom package to extract key information out of the model objects in tidy formats.
The `tidy()` function returns the parameter estimates of a `lm` object. Explain the relationship between the predictors and the response variable. Is the interpretation of `research`, "the higher the value of research indicates higher value of rank"? This doesn't make sense, why?

`broom`'s 3 very commonly used functions
- `tidy()`: extract model fit coefficients into a tidy table
- `glance()`: extracts model fit summary statistics 
- `augment()`: augments our data set to include fitted(predicted) & residuals among others

```{r}
broom::tidy(lm_fit)
```

$$y = \beta_0 + \beta_1.x_1 + ... + \epsilon$$

$$rank = 6.10 + 0.565.research + 0.091.student - 0.057diversity + \epsilon$$

*The coefficient for research is* `r tidy(lm_fit)[2,2] %>% pull()` *which says that as the value of research increases by 1, the rank increases by about a half.*

*Wait! Shouldn't higher research score indicate smaller rank, because low values of rank indicate better. The research variable is a rank also, so low values mean lots of research and high values being less research. It is a rank based on other variables collected, related to research activity, like number of publications and number of citations. Making plots ot these variables against research rank should show that they have a negative association.*

### 7

Make a dot and whisker plot of the coefficients, to visualise the significance of the different variables. Explain what you learn about the importance of the three explanatory variables for predicting the response.

```{r}
broom::tidy(lm_fit) %>% # extract model coefficients in a tidy tibble
  dotwhisker::dwplot(
    # set dot arguments
    dot_args = list(size = 2, 
                    color = "red"),
    # set whisker argument (for confidence intervals)
    whisker_args = list(color = "black"),
    # add vertical line at 0
    vline = geom_vline(xintercept = 0, 
                       colour = "grey50",
                       linetype = 2)) +
  # optional
  expand_limits(x = c(-0.75, 0.75)) + # expand limits; so 0 in middle
  scale_x_continuous(breaks = seq(from = -0.75, to = 0.75, by = 0.25)) # set breaks
```

*The only important variable contributing to the model is research, because the confidence intervals for the other two overlap with 0, and the hypothesis test measuring  difference from 0 is only significant for research. The coefficient for research is positive, meaning that the higher the research score the higher the rank.*

### 8

Report the fit statistics, using `broom::glance()`. What do you learn about the strength of the fit?

```{r}
# extract model summary; in tidy `tibble`
broom::glance(lm_fit)
```

*The model explains about half the variation in rank, because $R^2 is 0.485. That is, it is a moderately well-fitting model.*

### 9

Explore the model fit visually. Plot the predicted values against observed, residuals against fitted, and predicted against each of the predictors. Summarise what you learn about the model fit.

```{r}
# === plot the fit

# --- augment data set
nrc_all <- broom::augment(lm_fit, nrc)

# --- response vs. predicted 
p1 <- nrc_all %>% 
  ggplot(aes(x = .pred,
             y = rank)) +
  geom_point()

# --- predicted vs. residuals
p2 <- nrc_all %>% 
  ggplot(aes(x = .pred, 
             y = .resid)) +
  geom_point()

# patchwork
require(patchwork)
p1 + p2
```

*Observed vs predicted shows the fit is reasonably good. There are two outliers, revealed more by the residual plot. These are two programs that poor ranking, but predicted to be much better.*

```{r fig.height=3}
# --- plot predicted values vs. independent variables
p3 <- nrc_all %>% 
  ggplot(aes(x = research,
             y = .pred)) +
  geom_point()

p4 <- nrc_all %>% 
  ggplot(aes(x = student,
             y = .pred)) + 
  geom_point()

p5 <- nrc_all %>% 
  ggplot(aes(x = diversity,
             y = .pred)) +
  geom_point()

# patchwork
require(patchwork)
p3 + p4 + p5
```

*There is a very strong relationship between research and predicted values, which further supports that the model is primarily using research as the predictor.*

### 10 

Generate a grid of new data values to predict, with all combinations of `research = c(10, 40, 70)`, `student = c(10, 40, 70)`, `diversity = c(10, 40, 70)`. Predict these values, as point and confidence intervals. 

```{r}
# === predict new data (simulated)

# --- create new data points
new_points <- base::expand.grid(research = c(10, 40, 70), 
                          student = c(10, 40, 70),
                          diversity = c(10, 40, 70))
# OR
new_points <- tidyr::expand_grid(research = c(10, 40, 70),
                   student = c(10, 40, 70),
                   diversity = c(10, 40, 70))
new_points

# --- mean predictions
mean_pred <- stats::predict(object = lm_fit, 
                            new_data = new_points)
mean_pred

# --- confidence interval
conf_int_pred <- stats::predict(lm_fit, 
                                new_data = new_points, 
                                type = "conf_int") # conf. interval
conf_int_pred
```

### 11

Make a plot of predicted values vs research for the observed data and the new data, with new data coloured differently. How do the predicted values compare?

```{r}
# --- broom::augment; include predicted values
new_points <- broom::augment(lm_fit, new_points)

# --- plot predicted data vs `research`
ggplot() + 
  # for predicted vs. observed data
  geom_point(aes(x = research, 
                 y = .pred),
             data = nrc_all) +
  # for new data
  geom_point(aes(x = research, 
                 y = .pred), 
             colour="red",
             data = new_points)
```

*The new points are at just three values of research, so you can see the vertical stripes here. The predicted ranks for these points are more varied than the observed data. Its likely that this is due to the combination of values in the new data being different than what exists in the observed data. It would be a good idea to plot the predictors against eaach other to confirm that this is true.*

